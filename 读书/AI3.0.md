# AI3.0

先是“人工智能的春天”，紧接着是过度的承诺和媒体炒作，接下来便是“人工智能的寒冬”。从某种程度上来说，这一现象以5～10年为周期在不断上演

## 名词

深度神经网络: deep neural network, DNN

多层神经网络: multilayer neural network, MNN

卷积神经网络: convolutional neural network, CNN

深度学习: 是指用于训练DNN的算法,这里的DNN就是深度神经网络，指的是具有不止一个隐藏层的神经网络,深度学习中的“深度”并不是指神经网络所学习内容的复杂性，而仅仅是指网络本身的层数

依赖于收集到的大量已标注的数据来进行训练是深度学习不同于人类学习的另一个特点

监督学习: supervised learning
在训练时，给定学习系统一个样本，它就产生一个输出，然后在这时给它一个“监督信号”，提示它此输出与正确的输出有多大偏离，然后，系统会根据这个信号来调整它的权重和阈值。

现实世界中的大部分事件通常是可预测的，但仍有一长串低概率的意外事件发生。如果我们单纯依靠监督学习来提升人工智能系统对世界的认识，那么就会存在一个问题：尾部的情况并不经常出现在训练数据中，所以当遇到这些意外情况时，系统就会更容易出错

强化学习

在其最纯粹的形式下，强化学习不需要任何被标记的训练样本。代替它的是一个智能体，即学习程序，在一种特定环境（通常是计算机仿真环境）中执行一些动作，并偶尔从环境中获得奖励，这些间歇出现的奖励是智能体从学习中获得的唯一反馈

反向传播: back-propagation
反向传播算法是一种对输出端观察到的错误进行反向罪责传播，从而为网络中的每个权重都分配恰当罪责的方法。反向罪责传播是指，从右向左追溯罪责源头。这使得神经网络能够确定为减少错误应该对每个权重修改多少。神经网络中所谓的学习就是逐步修改连接的权重，从而使得每个输出在所有训练样本上的错误都尽可能接近于零

## 符号人工智能

符号人工智能最初是受到数学逻辑以及人们描述自身意识思考过程的方式的启发

符号人工智能的支持者认为，想要在计算机上获得智能，**并不需要构建模仿大脑运行的程序**。相反，其观点是，通用智能完全可以通过正确的符号处理程序(函数)来获得。

3个传教士和3个食人者都需要过河，但一艘小船上只能载2人。如果河岸一边饥饿的食人者的数量超过了传教士的话,传教士就会被吃掉那么，他们如何成功地渡过这条河？

## 亚符号人工智能

亚符号人工智能方法则从神经科学中汲取灵感，并试图捕捉隐藏在所谓的“快速感知”背后的一些无意识的思考过程，如识别人脸或识别语音等

亚符号人工智能程序不包含我们在前文的传教士和食人者的例子中看到的那种人类可理解的语言。与之相反，一个亚符号人工智能程序本质上是一堆等式——通常是一大堆难以理解的数字运算

亚符号系统往往难以阐释，并且没人知道如何直接将复杂的人类知识和逻辑编码到这些系统中。亚符号系统似乎更适合那些人类难以定义其中规则的感知任务。例如，你很难写出能够完成识别手写数字、接住棒球或识别你母亲声音等任务的规则，而你基本上是连下意识的思考都没有经过就自动完成了这些事情。

可以用符号系统来完成类似于语言描述和逻辑推理的高级任务，而用亚符号系统来完成诸如识别人脸和声音这样的低级感知任务

# AI历程

IBM的深蓝系统在1997年击败国际象棋世界冠军加里·卡斯帕罗夫
2011年，IBM的智能程序沃森在电视益智竞赛节目《危险边缘》中完胜人类冠军
2011年，AlphaGo在5局比赛的4局里击败了世界上最好的棋手之一


# AI的不足

## 不通用

AlphaGo可能是世界上最好的围棋玩家，但除此之外什么也做不了，它甚至不会玩跳棋、井字棋等游戏。

谷歌翻译可以把英文的影评翻译成中文，但它无法告诉你影评者是否喜欢这部电影，更不用说让它自己来观看和评论电影了。

随着时间的推移，人工智能领域的工作开始聚焦于特定的、定义明确的任务，如语音识别、下棋、自动驾驶等。

创造能执行这些功能的机器很有用并且往往利润丰厚，可以说这些任务中的任何一个都需要某种具体的智能，但至今人们还没有创建出任何能够在通用意义上被称为“智能”的人工智能程序。

该领域2016年的一项研究表明：“一堆狭义智能永远也不会堆砌成一种通用人工智能。通用人工智能的实现不在于单个能力的数量，而在于这些能力的整合。”

## 过拟合

如果ConvNets是在从网络下载的图像（如ImageNet中的图像）上进行训练的，那么在由机器人用照相机在房屋中移动拍摄出来的图像上，它们就会表现得很差

图像表面的变化，如使图像模糊一点或给图像加上斑点、更改某些颜色或场景中物体的旋转方向等，这些扰动不影响人类对其中对象的识别，却可能导致ConvNets出现严重错误

## 偏见

2015年，谷歌在它的照片应用程序中推出照片自动标注功能（应用了ConvNets），随后遭遇了一场公关噩梦。除了能够正确地使用诸如“飞机”“汽车”和“毕业生”之类的通用描述来标注图像之外，神经网络还用“大猩猩”标注了一张两名非裔美国人的自拍照。

在由深度学习驱动的视觉系统中，这种由种族或性别歧视导致的错误虽然不易察觉，却经常被观测到。例如，商业人脸识别系统识别男性白人的脸要比识别女性或非白人的脸更加准确

## 推导过程

ConvNets通过实施一系列在多隐藏层间传播的数学运算（卷积）来判定输入图像中包含的对象。

对于一个一般大小的网络，其运算可能会达到数十亿次，当然，对计算机进行编程，让它打印出一个网络对于给定输入所执行的全部加法和乘法的操作列表是很容易的，但是这样一个列表并不能使人类获知网络是如何得出答案的。

一个10亿次运算的列表不是一个普通人能接受的解释，即使是训练深度网络的人通常也无法理解其背后隐藏的原理，并为网络做出的决策提供解释

如果我们不理解DNN如何解答问题，我们就无法真正相信它们，或预测它们会在哪种情况下出错。

需要 **可解释的人工智能**

## 对抗攻击

在图像识别的技术上,对图像进行极小的、非常具体的变化使这张图像扭曲。扭曲后的图像对人类来说看起来毫无变化，但却被图像识别AI以高置信度归类为完全不同的东西

通过遗传算法创建的专门用于欺骗ConvNet的图像,在人类看起来像**噪音**,却能被图像识别AI精准识别为特定物体

有个研究团队开发了一个能够设计出具有特定图案的眼镜框的程序，愚弄了一个人脸识别系统，使其自信地将眼镜框的佩戴者错误地识别为另外一个人

另一个研究团队设计了可放置于交通标志上的不显眼的小贴纸，导致一个基于ConvNets的视觉系统（类似于自动驾驶汽车中使用的视觉系统）对交通标志进行了错误的分类，例如，将一个停车标志识别为限速标志

还有一个团队证实了用于医学图像分析的DNN可能面临的一种对抗式攻击：以一种人类难以察觉但却会引发网络改变对图像的分类的方式来改变X射线或显微镜图像，是不难做到的，但却可能会导致网络对该图像的判定完全相反，比如说，从以99%置信度显示目标图像分类为无癌症，到以99%置信度显示存在癌症26。该组人员指出，此类攻击手段可能会被医院人员或其他人用于制造欺诈性诊断，以便向保险公司索取额外的诊断测试费用

## 人脸识别

隐私问题是人脸识别技术应用中一个显而易见的问题。

即便我不使用Facebook或任何其他具有人脸识别功能的社交媒体平台，我的照片也可能会在未经我允许的情况下被标记并随后在网上被自动识别

失去隐私并不是唯一的风险，人们对于人脸识别还有一个更大的担忧，那就是可靠性：人脸识别系统会犯错。如果你的脸被错误匹配，你可能会被禁止进入一家商店、搭乘一架航班，或被错误地指控为一名罪犯。更重要的是，目前的人脸识别系统已经被证明对有色人种进行识别时明显比对白人的识别错误率更高

# AI

## 图灵测试

有两名被测试者,一台计算机和一个人，每名被测试者都被一个裁判员（人类）单独询问以试图判断哪个是人类、哪个是计算机。
裁判员与两名被测试者物理隔开，故他无法依靠视觉或听觉线索做出判断，只能靠打字交流。

图灵没有具体指定人类参赛者和裁判的选择标准，也没有规定测试应该持续多长时间，或者什么样的对话主题是被允许探讨的

## 监管

人工智能技术应该受到某种监管，但是监管不应该仅仅掌握在人工智能研究人员和相关公司的手里。

围绕人工智能的问题，比如可信度、可解释性、偏见、易受攻击性和使用过程中出现的道德问题，与技术问题一样，都是牵涉社会和政治方面的问题

欧盟议会在2018年颁布了一项关于人工智能的法规，有些人称之为“解释权”。这项法规要求，在“自动决策制定”的情况下，任何一个影响欧盟公民的决策都需要提供其中所涉及的与逻辑有关的有意义信息，并且这些信息需要使用清晰明了的语言，以简洁、透明、易懂和易于访问的形式来沟通和传达

## 新机器人三定律

1．有用的人工智能

在考虑人工智能在我们社会中的作用时，我们很容易把注意力集中在不利的一面，但是，有必要记住，人工智能系统已经为社会带来了巨大好处，并且它们有潜力发挥更大的作用。

2．可解释的人工智能

在人工智能“自动决策制定”的情况下，任何一个影响公民的决策都需要提供其中所涉及的与逻辑有关的有意义信息，并且这些信息需要使用清晰明了的语言，以简洁、透明、易懂和易于访问的形式来沟通和传达，这打开了有关解释问题的闸门。

3．可信的人工智能

在赋予计算机“道德智能”方面的进展不能与其他类型智能的进展分开，真正的挑战是创造出能够真正理解它们所面临的场景的机器。换句话说，可信任的道德理性的一个先决条件是通用的常识，而这，正如我们所见，即使在当今最好的人工智能系统中也是缺失的。